# 前言

这里主要针对安利进行分析讲解，欢迎大家在 issue 或是直接 contribution 更多相关的海量数据相关难题。这里我将持续性的更新一些面试中常见的海量数据案例。



海量数据问题处理方法

- Hash
- Bit-Map
- 布隆过滤器 (Bloom Filter)
- 堆 (Heap)
- 双层桶划分
- 数据库索引
- 倒排索引 (Inverted Index)
- B+树
- Trie树
- MapReduce 



# 海量数据案例

## #1 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

**方案1**：可以估计每个文件安的大小为 5G×64 =320G，远远大于内存限制的 4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

遍历文件 a，对每个 url 求取 hash(url) ，然后根据所取得的值将 url 分别存储到 1000 个小文件（记为a0,a1,...,a999）中。这样每个小文件的大约为 300M。

遍历文件 b，采取和 a 相同的方式将 url 分别存储到 1000 小文件（记为 b0,b1,...,b999 ）。这样处理后，所有可能相同的 url 都在对应的小文件（a0 vs b0,a1vs b1,...,a999 vs b999）中，不对应的小文件不可能有相同的 url。然后我们只要求出 1000 对小文件中相同的 url 即可。

求每对小文件中相同的 url 时，可以把其中一个小文件的 url 存储到 hash_set 中。然后遍历另一个小文件的每个 url，看其是否在刚才构建的 hash_set 中，如果是，那么就是共同的 url，存到文件里面就可以了。



**方案2**：如果允许有一定的错误率，可以使用 Bloom filter，4G 内存大概可以表示 340 亿 bit。将其中一个文件中的 url 使用 Bloom filter 映射为这 340 亿 bit，然后挨个读取另外一个文件的 url，检查是否与 Bloom filter，如果是，那么该 ur l应该是共同的 url（注意会有一定的错误率）。



参考资料：

- [搞定海量数据处理，六道海量数据处理面试题分析与十大方法总结](https://zhuanlan.zhihu.com/p/40430913)
- [海量数据处理：十道面试题与十个海量数据处理方法总结 - chenhuan001 - 博客园](https://www.cnblogs.com/chenhuan001/p/5866916.html)